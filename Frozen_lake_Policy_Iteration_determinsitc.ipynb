{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Policy Iteration with Frozen Lake</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, you will use Perform Policy Iteration on the FrozenLake environment on open AI gym </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#PE\">Policy Evaluation</a></li>\n",
    "    <li><a href=\"#PI\">Policy Improvement  </a></li>\n",
    "    <li><a href=\"#Result\">Policy Iteration</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda seaborn\n",
    "\n",
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "environment= gym.make('FrozenLake-v0')\n",
    "environment.reset()\n",
    "environment.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name=[['S','F','F','F'],\n",
    "['F','H','F','H'],\n",
    "['F','F','F','H'],\n",
    "['H','F','F','G']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>S : starting point, safe</p>\n",
    "<p>F : frozen surface, safe</p>\n",
    "<p>H : hole, fall to your doom</p>\n",
    "<p>G : goal, where the frisbee is located</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>LEFT = 0</p>\n",
    "<p>DOWN = 1</p>\n",
    "<p>RIGHT = 2</p>\n",
    "<p>UP = 3</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(\"{},start{},stop{}\".format(i,i%4,i%4+1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a frozen lake environment object, render it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Environment=environment.env\n",
    "Environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the number of states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Environment.nS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also obtain the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Environment.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot out the states as a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrawStateGrid():\n",
    "    def __init__(self,Environment,v=None):\n",
    "        state_number=4\n",
    "        self.state_name=[['S','F','F','F'],\n",
    "        ['F','H','F','H'],\n",
    "        ['F','F','F','H'],\n",
    "        ['H','F','F','G']]\n",
    "        self.numbber=[[0,1,2,3],\n",
    "        [4,5,6,7],\n",
    "        [8,9,10,11],\n",
    "        [12,13,14,15]]\n",
    "\n",
    "        self.state_action_state=np.zeros((Environment.nS,Environment.nA))\n",
    "        \n",
    "        \n",
    "        for state in range(Environment.nS):\n",
    "            for action in range(Environment.nA):\n",
    "                if len(Environment.P[state][action])>1:\n",
    "                    self.state_action_state[state,action]=Environment.P[state][action][1][1]\n",
    "                else: \n",
    "                       self.state_action_state[state,action]=Environment.P[state][action][0][1]\n",
    "        \n",
    "        major_ticks=[i+1 for i in range(state_number)]\n",
    "        minor_ticks=[i for i in range(state_number)]\n",
    "        self.fig,self.ax = plt.subplots()\n",
    "\n",
    "\n",
    "        self.ax.set_xlim([0,state_number])\n",
    "        self.ax.set_ylim([0,state_number])\n",
    "        self.ax.grid()\n",
    "        self.ax.set_xticks(major_ticks)\n",
    "        self.ax.set_xticks(minor_ticks, minor=True)\n",
    "        self.ax.set_yticks(major_ticks)\n",
    "        self.ax.set_yticks(minor_ticks, minor=True)\n",
    "        n=0 \n",
    "        for i,row in enumerate(self.state_name):\n",
    "            for j,val in enumerate(row): \n",
    "                self.ax.text(j,4-i-1,\"{}:{}\".format(val,str(n)),size=25)\n",
    "           \n",
    "                n+=1\n",
    "        \n",
    "    \n",
    "    def plot_state(self,start_state,end_state,action):\n",
    "\n",
    "        #start_state=0\n",
    "        #end_state=1\n",
    "        #action=0\n",
    "        #map states to position on table, top line on square subtract one and its the bottom line\n",
    "        fig2 = plt.figure()\n",
    "        y_top=[4,4,4,4,3,3,3,3,2,2,2,2,1,1,1,1]\n",
    "        \n",
    "        action_arrow=[(0.5,0),(0,-0.5),(-0.5,0),(0,0.5)]\n",
    "        \n",
    "        x_start=np.linspace(start_state%4,start_state%4+1,10)\n",
    "        self.ax.fill_between(x_start,y_top[start_state]-1,y_top[start_state],color='b')\n",
    "        \n",
    "        x_stop=np.linspace(end_state%4,end_state%4+1,10)\n",
    "\n",
    "        self.ax.fill_between( x_stop,y_top[end_state]-1,y_top[end_state],color='r')\n",
    "        \n",
    "     \n",
    "        self.ax.arrow(x_start[5], y_top[start_state]-0.5, action_arrow[action][0], action_arrow[action][1],head_width=0.05, head_length=0.1, fc='k', ec='k') \n",
    "        \n",
    "    def plot_policy(self,policy):\n",
    "   \n",
    "    \n",
    "        action_arrow=[(-0.5,0),(0,-0.5),(0.5,0),(0,0.5)]\n",
    "        y_top=[4,4,4,4,3,3,3,3,2,2,2,2,1,1,1,1]    \n",
    "        for state,action_set in enumerate(policy):\n",
    "            action=np.argmax(action_set)\n",
    "            if max(action_set)==1:\n",
    "                x_start=np.linspace(state%4,state%4+1,10)\n",
    "                self.ax.arrow(x_start[5], y_top[state]-0.5, action_arrow[action][0], action_arrow[action][1],head_width=0.05, head_length=0.1, fc='k', ec='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can draw the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawStateGrid(Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"PE\">Policy Evaluation</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an empty array for the 1-D value function and a  2D array for the policy. The policy will randomly select an action with equal Probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=np.zeros(Environment.nS)\n",
    "policy=np.ones((Environment.nS,Environment.nA))/Environment.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fucion will iteratively calculate the value function for the following policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{V^{k+1}}_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\sum_{s' \\in \\mathcal{S}}  \\mathcal{P}_{ss'}^a \\big[\\mathcal{R}_s^a + \\gamma  {V^{k}}_{\\pi}(s')\\big]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#env, policy, gamma=1., theta=1e-8\n",
    "def policy_evaluation(Environment,policy,max_iterations=100,discount_factor=1,theta=1e-9,record=False):\n",
    "    V=np.zeros(Environment.nS)\n",
    "    if record:\n",
    "        delta_iteration=[]\n",
    "        V_iterations=[[0 ] for i in range(Environment.nS) ]\n",
    "        \n",
    "    else:\n",
    "        delta_iteration=None\n",
    "        V_iterations=None\n",
    "        \n",
    "\n",
    "    discount_factor=1\n",
    "\n",
    "    # for each iteration\n",
    "    for iteration  in range(max_iterations):\n",
    "        #stors the largest difference between states \n",
    "        delta = 0\n",
    "        for state in range(Environment.nS):\n",
    "            v=0\n",
    "            for action,action_probability in enumerate(policy[state]):\n",
    "                for state_probability, next_state, reward, terminated in Environment.P[state][action]:\n",
    "                    v+=action_probability*state_probability*(reward+discount_factor*V[next_state])\n",
    "                # find the largest difference between previous state estimate and present state estimate \n",
    "        \n",
    "            delta=max(delta,v-V[state])\n",
    "            if record:\n",
    "                V_iterations[state].append(V[state])\n",
    "                delta_iteration.append(delta)\n",
    "            \n",
    "            V[state]=v\n",
    "    \n",
    "    \n",
    "        if delta<theta:\n",
    "            break\n",
    "            \n",
    "    if record:\n",
    "        return V, V_iterations, delta_iteration\n",
    "    else:\n",
    "        return V \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations=100\n",
    "discount_factor=1\n",
    "theta=1e-9\n",
    "V, V_iterations, delta_iteration=policy_evaluation(Environment,policy,max_iterations=100,discount_factor=1,theta=1e-9,record=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will plot the value function for each iteration. It will also track the smallest change between each iteration and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "for s,v_iteration in enumerate(V_iterations):\n",
    "    \n",
    "    plt.plot(v_iteration,label=\"Return for state {}\".format(s))\n",
    "\n",
    "plt.ylabel('value for that state')\n",
    "plt.xlabel('iterations k')\n",
    "plt.subplot(212)\n",
    "plt.plot(delta_iteration )\n",
    "plt.plot(theta* np.ones(s))\n",
    "plt.yscale('log')\n",
    "plt.ylabel('delta log')   \n",
    "plt.xlabel('iterations k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the value function as a grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = np.asarray([['V(0)=','V(1)=','V(2)=','V(3)='],['V(4)=','V(5)=','V(6)=','V(7)='],['V(8)=','V(9)=','V(10)=','V(11)='],['V(12)=','V(13)=','V(14)=','V(15)=']])\n",
    "labels =(np.asarray([\"{}{}\".format(text,str(v)[0:4]) for v,text in zip(V,Text.flatten())]).reshape(4,4))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(V.reshape(4, 4),annot=labels,  cmap=\"YlGnBu\", cbar=False,fmt='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"PI\">Policy Improvement</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function for the lookahead step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_{\\pi}(a,s)=\\sum_{s' \\in \\mathcal{S}}  \\mathcal{P}_{ss'}^a \\big[\\mathcal{R}_s^a + \\gamma  {V^{k}}_{\\pi}(s')\\big]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_ahead(Environment, V,discount_factor=0.9):\n",
    "    Q=np.zeros((Environment.nS,Environment.nA))\n",
    " \n",
    "    for state in range(Environment.nS):\n",
    "\n",
    "        for action in range(Environment.nA):\n",
    "            q=0\n",
    "            for state_probability, next_state, reward, terminated in Environment.P[state][action]: \n",
    "                q+=state_probability*(reward+discount_factor*V[next_state])\n",
    "           \n",
    "            Q[state,action]=q\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the lookahead step and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=look_ahead(Environment, V)   \n",
    "plt.figure(figsize=(5, 16))\n",
    "sns.heatmap(Q,  cmap=\"YlGnBu\", annot=True, cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function the Performs Policy Improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_impovment(Q,policy): \n",
    "    new_policy=policy\n",
    "    N_actions=new_policy[0,:].shape[0]\n",
    "\n",
    "    not_terminal_states=np.sum(Q,axis=1)!=0\n",
    "    best_actions=np.argmax(Q[:,:],axis=1)\n",
    "\n",
    "    for state,(not_terminal,best_action) in enumerate(zip(not_terminal_states,best_actions)):\n",
    "            if not_terminal==True:\n",
    "                new_policy[state] =np.eye(N_actions)[best_action]\n",
    "            else:\n",
    "                 new_policy[state]=policy[state]\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy=policy_impovment(Q,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 16))\n",
    "sns.heatmap(new_policy,  cmap=\"YlGnBu\", annot=True, cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions perform the same task but work with deterministic froze lake [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_value(env, V, s, gamma=0.9):\n",
    "    r\"\"\"Q-value (action-value) function from state-value function\n",
    "\n",
    "    Returns Q values, values of actions.\n",
    "\n",
    "    Args:\n",
    "        env (gym.env): OpenAI environment class instantiated and assigned to an object.\n",
    "        V (np.array): array of state-values obtained from policy evaluation function.\n",
    "        s (integer): integer representing current state in the gridworld\n",
    "        gamma (float): discount rate for rewards.\n",
    "    \"\"\"\n",
    "    # 1. Create q-value array for one state\n",
    "    # We have 4 actions, so let's create an array with the size of 4\n",
    "    A_n=env.nA\n",
    "    q = np.zeros(A_n)\n",
    "\n",
    "    # 2. Loop through each action\n",
    "    for a in range(A_n):\n",
    "        # 2.1 For each action, we've our transition probabilities, next state, rewards and whether the game ended\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            # 2.1.1 Get our action-values from state-values\n",
    "            q[a] += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "    # Return action values\n",
    "    return q\n",
    "\n",
    "\n",
    "\n",
    "def policy_improvement(env, V, gamma=1):\n",
    "\n",
    "    # 1. Blank policy\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "\n",
    "    # 2. For each state in 16 states\n",
    "    for s in range(env.nS):\n",
    "\n",
    "        # 2.1 Get q values: q.shape returns (4,)\n",
    "        q = q_value(env, V, s, gamma=0.9)\n",
    "\n",
    "\n",
    "        best_a = np.argwhere(q==np.max(q)).flatten()\n",
    "\n",
    "        # 2.3 One-hot encode be\n",
    "        \n",
    "        \n",
    "        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n",
    "\n",
    "    return policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"PI\">Policy Iteration</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration we can now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def policy_iteration(env, discount_factor=0.9, theta=1e-9,gamma=1.):\n",
    "    # 1. Create equiprobable policy where every state has 4 actions with equal probabilities as a starting policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "    # 2. Loop through policy_evaluation and policy_improvement functions\n",
    "    while True:\n",
    "        # 2.1 Get state-values\n",
    "        V=policy_evaluation(Environment,policy,max_iterations=100,discount_factor=gamma,theta=1e-9)\n",
    "      \n",
    "            \n",
    " \n",
    "        # 2.2 Get new policy by getting q-values and maximizing q-values per state to get best action per state\n",
    "        new_policy= policy_improvement(env, V)\n",
    "        #new_policy=policy_impovment(Q)\n",
    "        # 2.3 Stop if the value function estimates for successive policies has converged\n",
    "        if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta * 1e2:\n",
    "            break;\n",
    "\n",
    "        policy = new_policy\n",
    "    return policy, V ,Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform Policy Iteration  and plot out the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_pi, V_pi ,q_pi= policy_iteration(Environment)\n",
    "test=DrawStateGrid(Environment,V_pi)\n",
    "test.plot_policy(policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result seems strange;let's look at the value function and action function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(V_pi.reshape(4,4),annot=True, cbar=False)\n",
    "plt.title(\"Value function \")\n",
    "plt.show()\n",
    "sns.heatmap(q_pi,  cmap=\"YlGnBu\", annot=True, cbar=False)\n",
    "plt.title(\"Action function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand, let's look at the portability of each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Environment.P[14][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Environment.P[14][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also determine the best policy for the deterministic case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deterministicfrozenlake import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_pi, V_pi ,q_pi= policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=DrawStateGrid(Environment,V_pi)\n",
    "test.plot_policy(policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>About the Authors:</b> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>References</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol type=\"1\">\n",
    "    <li>Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction.\" 2011></li>\n",
    "    <li><a href=\"https://www.deeplearningwizard.com/deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/\">Dynamic Programming-Deep Learning Wizard</a></li>\n",
    "    <li><a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">FrozenLake-v0</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
